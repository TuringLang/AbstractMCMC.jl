var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"AbstractMCMC defines an interface for sampling Markov chains.","category":"section"},{"location":"api/#Model","page":"API","title":"Model","text":"","category":"section"},{"location":"api/#Sampler","page":"API","title":"Sampler","text":"","category":"section"},{"location":"api/#Sampling-a-single-chain","page":"API","title":"Sampling a single chain","text":"","category":"section"},{"location":"api/#Iterator","page":"API","title":"Iterator","text":"","category":"section"},{"location":"api/#Transducer","page":"API","title":"Transducer","text":"","category":"section"},{"location":"api/#Sampling-multiple-chains-in-parallel","page":"API","title":"Sampling multiple chains in parallel","text":"Two algorithms are provided for parallel sampling with multiple threads and multiple processes, and one allows for the user to sample multiple chains in serial (no parallelization):","category":"section"},{"location":"api/#Common-keyword-arguments","page":"API","title":"Common keyword arguments","text":"tip: Turing.jl\nThe description of keyword arguments here is fairly generic, since it only pertains to the AbstractMCMC interface. If you are specifically interested in sampling with Turing.jl, there is a dedicated documentation page for that.\n\nCommon keyword arguments for regular and parallel sampling are:\n\nprogress (default: AbstractMCMC.PROGRESS[] which is true initially): toggles progress logging. See the section on Progress logging below for more details.\nchain_type (default: Any): determines the type of the returned chain.\ncallback (default: nothing): if callback !== nothing, then callback(rng, model, sampler, sample, iteration; kwargs...) is called after every sampling step, where sample is the most recent sample of the Markov chain and iteration is the current iteration.\nKeyword arguments kwargs... are passed down from the call to sample(...). If you are performing multiple-chain sampling, then kwargs additionally contains a chain_number keyword argument, which runs from 1 to the number of chains. This is not present when performing single-chain sampling.\nnum_warmup (default: 0): number of \"warm-up\" steps to take before the first \"regular\" step,  i.e. number of times to call AbstractMCMC.step_warmup before the first call to  AbstractMCMC.step.\ndiscard_initial (default: num_warmup): number of initial samples that are discarded. Note that if discard_initial < num_warmup, warm-up samples will also be included in the resulting samples.\nthinning (default: 1): factor by which to thin samples.\ninitial_state (default: nothing): if initial_state !== nothing, the first call to AbstractMCMC.step is passed initial_state as the state argument.\n\ninfo: Info\nThe common keyword arguments progress, chain_type, and callback are not supported by the iterator AbstractMCMC.steps and the transducer AbstractMCMC.Sample.\n\nThere is no \"official\" way for providing initial parameter values yet. However, multiple packages such as EllipticalSliceSampling.jl and AdvancedMH.jl support an initial_params keyword argument for setting the initial values when sampling a single chain. To ensure that sampling multiple chains \"just works\" when sampling of a single chain is implemented, we decided to support initial_params in the default implementations of the ensemble methods:\n\ninitial_params (default: nothing): if initial_params isa AbstractArray, then the ith element of initial_params is used as initial parameters of the ith chain. If one wants to use the same initial parameters x for every chain, one can specify e.g. initial_params = FillArrays.Fill(x, N).","category":"section"},{"location":"api/#Progress-logging","page":"API","title":"Progress logging","text":"Progress logging is controlled in one of two ways:\n\nby passing the progress keyword argument to the sample(...) function, or\nby globally changing the defaults with AbstractMCMC.setprogress! and AbstractMCMC.setmaxchainsprogress!.","category":"section"},{"location":"api/#progress-keyword-argument","page":"API","title":"progress keyword argument","text":"For single-chain sampling (i.e., sample([rng,] model, sampler, N)), as well as multiple-chain sampling with MCMCSerial, the progress keyword argument should be a Bool.\n\nFor multiple-chain sampling using MCMCThreads, there are several, more detailed, options:\n\n:overall: create one progress bar for the overall sampling process, which tracks the percentage of samples that have been sampled across all chains\n:perchain: in addition to :overall, also create one progress bar for each individual chain\n:none: do not create any progress bar\ntrue (the default): same as :overall, i.e. one progress bar for the overall sampling process\nfalse: same as :none, i.e. no progress bar\n\nMultiple-chain sampling using MCMCDistributed behaves the same as MCMCThreads, except that :perchain is not (yet?) implemented.\n\nwarning: Do not override the `progress` keyword argument\nIf you are implementing your own methods for sample(...), you should make sure to not override the progress keyword argument if you want progress logging in multi-chain sampling to work correctly, as the multi-chain sample() call makes sure to specifically pass custom values of progress to the single-chain calls.","category":"section"},{"location":"api/#Global-settings","page":"API","title":"Global settings","text":"If you are sampling multiple times and would like to change the default behaviour, you can use this function to control progress logging globally:\n\nsetprogress! is more general, and applies to all types of sampling (both single- and multiple-chain). It only takes a boolean argument, which switches progress logging on or off. For example, setprogress!(false) will disable all progress logging.\n\nNote that setprogress! cannot be used to set the type of progress bar for multiple-chain sampling. If you want to use :perchain, it has to be set on each individual call to sample.","category":"section"},{"location":"api/#Chains","page":"API","title":"Chains","text":"The chain_type keyword argument allows to set the type of the returned chain. A common choice is to return chains of type Chains from MCMCChains.jl.\n\nAbstractMCMC defines the abstract type AbstractChains for Markov chains.\n\nThe following two functions exist for converting AbstractChains from and to matrices of samples.\n\nFor chains of this type, AbstractMCMC defines the following methods.","category":"section"},{"location":"api/#Interacting-with-states-of-samplers","page":"API","title":"Interacting with states of samplers","text":"To make it a bit easier to interact with some arbitrary sampler state, we encourage implementations of AbstractSampler to implement the following methods:\n\ngetparams and setparams!! provide a generic interface for interacting with the parameters of a sampler's state, regardless of how that state is represented internally.\n\nThis allows generic code to be written that works with any sampler implementing this interface. For example, a generic ensemble sampler could use getparams to extract the parameters from each of its component samplers' states, and setparams!! to initialize each component sampler with a different set of parameters.\n\nThe optional model argument to these functions allows sampler implementations to customize their behavior based on the model being used. For example, some samplers may need to evaluate the log density at new parameter values when setting parameters, which requires access to the model. If access to model is not needed, the sampler only needs to implement the version without the model argument - the default implementations will then call those methods directly.\n\nThese methods are particularly useful for implementing samplers which wrap some inner samplers, such as a mixture of samplers. In the next section, we will see how getparams and setparams!! can be used to implement a MixtureSampler.","category":"section"},{"location":"api/#Example:-MixtureSampler","page":"API","title":"Example: MixtureSampler","text":"In a MixtureSampler we need two things:\n\ncomponents: collection of samplers.\nweights: collection of weights representing the probability of choosing the corresponding sampler.\n\nstruct MixtureSampler{W,C} <: AbstractMCMC.AbstractSampler\n    components::C\n    weights::W\nend\n\nTo implement the state, we need to keep track of a couple of things:\n\nindex: the index of the sampler used in this step.\nstates: the current states of all the components.\n\nWe need to keep track of the states of all components rather than just the state for the sampler we used previously. The reason is that lots of samplers keep track of more than just the previous realizations of the variables, e.g. in AdvancedHMC.jl we keep track of the momentum used, the metric used, etc.\n\nstruct MixtureState{S}\n    index::Int\n    states::S\nend\n\nThe step for a MixtureSampler is defined by the following generative process\n\nbeginaligned\ni sim mathrmCategorical(w_1 dots w_k) \nX_t sim mathcalK_i(cdot mid X_t - 1)\nendaligned\n\nwhere mathcalK_i denotes the i-th kernel/sampler, and w_i denotes the weight/probability of choosing the i-th sampler. AbstractMCMC.getparams and AbstractMCMC.setparams!! comes into play in defining/computing mathcalK_i(cdot mid X_t - 1) since X_t - 1 could be coming from a different sampler.\n\nIf we let state be the current MixtureState, i the current component, and i_prev is the previous component we sampled from, then this translates into the following piece of code:\n\n# Update the corresponding state, i.e. `state.states[i]`, using\n# the state and transition from the previous iteration.\nstate_current = AbstractMCMC.setparams!!(\n    state.states[i], \n    AbstractMCMC.getparams(state.states[i_prev]),\n)\n\n# Take a `step` for this sampler using the updated state.\ntransition, state_current = AbstractMCMC.step(\n    rng, model, sampler_current, sampler_state;\n    kwargs...\n)\n\nThe full AbstractMCMC.step implementation would then be something like:\n\nfunction AbstractMCMC.step(rng, model::AbstractMCMC.AbstractModel, sampler::MixtureSampler, state; kwargs...)\n    # Sample the component to use in this `step`.\n    i = rand(Categorical(sampler.weights))\n    sampler_current = sampler.components[i]\n\n    # Update the corresponding state, i.e. `state.states[i]`, using\n    # the state and transition from the previous iteration.\n    i_prev = state.index\n    state_current = AbstractMCMC.setparams!!(  \n        state.states[i], \n        AbstractMCMC.getparams(state.states[i_prev]),  \n    )\n\n    # Take a `step` for this sampler using the updated state.\n    transition, state_current = AbstractMCMC.step(\n        rng, model, sampler_current, state_current;\n        kwargs...\n    )\n\n    # Create the new states.\n    # NOTE: Code below will result in `states_new` being a `Vector`.\n    # If we wanted to allow usage of alternative containers, e.g. `Tuple`,\n    # it would be better to use something like `@set states[i] = state_current`\n    # where `@set` is from Setfield.jl.\n    states_new = map(1:length(state.states)) do j\n        if j == i\n            # Replace the i-th state with the new one.\n            state_current\n        else\n            # Otherwise we just carry over the previous ones.\n            state.states[j]\n        end\n    end\n\n    # Create the new `MixtureState`.\n    state_new = MixtureState(i, states_new)\n\n    return transition, state_new\nend\n\nAnd for the initial AbstractMCMC.step we have:\n\nfunction AbstractMCMC.step(rng, model::AbstractMCMC.AbstractModel, sampler::MixtureSampler; kwargs...)\n    # Initialize every state.\n    transitions_and_states = map(sampler.components) do spl\n        AbstractMCMC.step(rng, model, spl; kwargs...)\n    end\n\n    # Sample the component to use this `step`.\n    i = rand(Categorical(sampler.weights))\n    # Extract the corresponding transition.\n    transition = first(transitions_and_states[i])\n    # Extract states.\n    states = map(last, transitions_and_states)\n    # Create new `MixtureState`.\n    state = MixtureState(i, states)\n\n    return transition, state\nend\n\nSuppose we then wanted to use this with some of the packages which implements AbstractMCMC.jl's interface, e.g. AdvancedMH.jl, then we'd simply have to implement getparams and setparams!!.\n\nTo use MixtureSampler with two samplers sampler1 and sampler2 from AdvancedMH.jl as components, we'd simply do\n\nsampler = MixtureSampler([sampler1, sampler2], [0.1, 0.9])\ntransition, state = AbstractMCMC.step(rng, model, sampler)\nwhile ...\n    transition, state = AbstractMCMC.step(rng, model, sampler, state)\nend","category":"section"},{"location":"api/#AbstractMCMC.AbstractModel","page":"API","title":"AbstractMCMC.AbstractModel","text":"AbstractModel\n\nAn AbstractModel represents a generic model type that can be used to perform inference.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractMCMC.LogDensityModel","page":"API","title":"AbstractMCMC.LogDensityModel","text":"LogDensityModel <: AbstractMCMC.AbstractModel\n\nWrapper around something that implements the LogDensityProblems.jl interface.\n\nNote that this does not implement the LogDensityProblems.jl interface itself, but it is simply useful for indicating to the sample and other AbstractMCMC methods that the wrapped object implements the LogDensityProblems.jl interface.\n\nFields\n\nlogdensity: The object that implements the LogDensityProblems.jl interface.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractMCMC.AbstractSampler","page":"API","title":"AbstractMCMC.AbstractSampler","text":"AbstractSampler\n\nThe AbstractSampler type is intended to be inherited from when implementing a custom sampler. Any persistent state information should be saved in a subtype of AbstractSampler.\n\nWhen defining a new sampler, you should also overload the function transition_type, which tells the sample function what type of parameter it should expect to receive.\n\n\n\n\n\n","category":"type"},{"location":"api/#StatsBase.sample-Tuple{AbstractRNG, AbstractMCMC.AbstractModel, AbstractMCMC.AbstractSampler, Any}","page":"API","title":"StatsBase.sample","text":"sample(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    model::AbstractModel,\n    sampler::AbstractSampler,\n    N_or_isdone;\n    kwargs...,\n)\n\nSample from the model with the Markov chain Monte Carlo sampler and return the samples.\n\nIf N_or_isdone is an Integer, exactly N_or_isdone samples are returned.\n\nOtherwise, sampling is performed until a convergence criterion N_or_isdone returns true. The convergence criterion has to be a function with the signature\n\nisdone(rng, model, sampler, samples, state, iteration; kwargs...)\n\nwhere state and iteration are the current state and iteration of the sampler, respectively. It should return true when sampling should end, and false otherwise.\n\nKeyword arguments\n\nSee https://turinglang.org/AbstractMCMC.jl/dev/api/#Common-keyword-arguments for common keyword arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{AbstractRNG, Any, AbstractMCMC.AbstractSampler, Any}","page":"API","title":"StatsBase.sample","text":"sample(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    logdensity,\n    sampler::AbstractSampler,\n    N_or_isdone;\n    kwargs...,\n)\n\nWrap the logdensity function in a LogDensityModel, and call sample with the resulting model instead of logdensity.\n\nThe logdensity function has to support the LogDensityProblems.jl interface.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractMCMC.steps-Tuple{AbstractRNG, AbstractMCMC.AbstractModel, AbstractMCMC.AbstractSampler}","page":"API","title":"AbstractMCMC.steps","text":"steps(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    model::AbstractModel,\n    sampler::AbstractSampler;\n    kwargs...,\n)\n\nCreate an iterator that returns samples from the model with the Markov chain Monte Carlo sampler.\n\nExamples\n\njulia> struct MyModel <: AbstractMCMC.AbstractModel end\n\njulia> struct MySampler <: AbstractMCMC.AbstractSampler end\n\njulia> function AbstractMCMC.step(rng, ::MyModel, ::MySampler, state=nothing; kwargs...)\n           # all samples are zero\n           return 0.0, state\n       end\n\njulia> iterator = steps(MyModel(), MySampler());\n\njulia> collect(Iterators.take(iterator, 10)) == zeros(10)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractMCMC.steps-Tuple{AbstractRNG, Any, AbstractMCMC.AbstractSampler}","page":"API","title":"AbstractMCMC.steps","text":"steps(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    logdensity,\n    sampler::AbstractSampler;\n    kwargs...,\n)\n\nWrap the logdensity function in a LogDensityModel, and call steps with the resulting model instead of logdensity.\n\nThe logdensity function has to support the LogDensityProblems.jl interface.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractMCMC.Sample-Tuple{AbstractRNG, AbstractMCMC.AbstractModel, AbstractMCMC.AbstractSampler}","page":"API","title":"AbstractMCMC.Sample","text":"Sample(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    model::AbstractModel,\n    sampler::AbstractSampler;\n    kwargs...,\n)\n\nCreate a transducer that returns samples from the model with the Markov chain Monte Carlo sampler.\n\nExamples\n\njulia> struct MyModel <: AbstractMCMC.AbstractModel end\n\njulia> struct MySampler <: AbstractMCMC.AbstractSampler end\n\njulia> function AbstractMCMC.step(rng, ::MyModel, ::MySampler, state=nothing; kwargs...)\n           # all samples are zero\n           return 0.0, state\n       end\n\njulia> transducer = Sample(MyModel(), MySampler());\n\njulia> collect(transducer(1:10)) == zeros(10)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractMCMC.Sample-Tuple{AbstractRNG, Any, AbstractMCMC.AbstractSampler}","page":"API","title":"AbstractMCMC.Sample","text":"Sample(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    logdensity,\n    sampler::AbstractSampler;\n    kwargs...,\n)\n\nWrap the logdensity function in a LogDensityModel, and call Sample with the resulting model instead of logdensity.\n\nThe logdensity function has to support the LogDensityProblems.jl interface.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{AbstractRNG, AbstractMCMC.AbstractModel, AbstractMCMC.AbstractSampler, AbstractMCMC.AbstractMCMCEnsemble, Integer, Integer}","page":"API","title":"StatsBase.sample","text":"sample(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    model::AbstractModel,\n    sampler::AbstractSampler,\n    parallel::AbstractMCMCEnsemble,\n    N::Integer,\n    nchains::Integer;\n    kwargs...,\n)\n\nSample nchains Monte Carlo Markov chains from the model with the sampler in parallel using the parallel algorithm, and combine them into a single chain.\n\nKeyword arguments\n\nSee https://turinglang.org/AbstractMCMC.jl/dev/api/#Common-keyword-arguments for common keyword arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{AbstractRNG, Any, AbstractMCMC.AbstractSampler, AbstractMCMC.AbstractMCMCEnsemble, Integer, Integer}","page":"API","title":"StatsBase.sample","text":"sample(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    logdensity,\n    sampler::AbstractSampler,\n    parallel::AbstractMCMCEnsemble,\n    N::Integer,\n    nchains::Integer;\n    kwargs...,\n)\n\nWrap the logdensity function in a LogDensityModel, and call sample with the resulting model instead of logdensity.\n\nThe logdensity function has to support the LogDensityProblems.jl interface.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractMCMC.MCMCThreads","page":"API","title":"AbstractMCMC.MCMCThreads","text":"MCMCThreads\n\nThe MCMCThreads algorithm allows users to sample MCMC chains in parallel using multiple threads.\n\nUsage\n\nsample(model, sampler, MCMCThreads(), N, nchains)\n\nSee also sample.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractMCMC.MCMCDistributed","page":"API","title":"AbstractMCMC.MCMCDistributed","text":"MCMCDistributed\n\nThe MCMCDistributed algorithm allows users to sample MCMC chains in parallel using multiple processes.\n\nUsage\n\nsample(model, sampler, MCMCDistributed(), N, nchains)\n\nSee also sample.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractMCMC.MCMCSerial","page":"API","title":"AbstractMCMC.MCMCSerial","text":"MCMCSerial\n\nThe MCMCSerial algorithm allows users to sample serially, with no thread or process parallelism.\n\nUsage\n\nsample(model, sampler, MCMCSerial(), N, nchains)\n\nSee also sample.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractMCMC.setprogress!","page":"API","title":"AbstractMCMC.setprogress!","text":"setprogress!(progress::Bool; silent::Bool=false)\n\nEnable progress logging globally if progress is true, and disable it otherwise.  Optionally disable informational message if silent is true.\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractMCMC.AbstractChains","page":"API","title":"AbstractMCMC.AbstractChains","text":"AbstractChains\n\nAbstractChains is an abstract type for an object that stores parameter samples generated through a MCMC process.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractMCMC.to_samples","page":"API","title":"AbstractMCMC.to_samples","text":"AbstractMCMC.to_samples(::Type{T}, chains::AbstractChains) where {T}\n\nConvert an AbstractChains object to an Matrix of parameter samples.\n\nMethods of this function should be implemented with the signature listed above, and should return a Matrix{T}.\n\nSee also: from_samples.\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractMCMC.from_samples","page":"API","title":"AbstractMCMC.from_samples","text":"AbstractMCMC.from_samples(::Type{T}, samples::Matrix) where {T<:AbstractChains}\n\nConvert a Matrix of parameter samples to an AbstractChains object.\n\nMethods of this function should be implemented with the signature listed above, and should return a chain of type T.\n\nThis function is the inverse of to_samples.\n\nIn general, it is expected that for chains::Tchn, from_samples(Tchn, to_samples(Tsample, chains)) contains data that are equivalent to that in chains (although differences in e.g. metadata or ordering are permissible).\n\nFurthermore, the same should hold true for to_samples(Tsample, from_samples(Tchn, samples)) where samples::Matrix{Tsample}.\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractMCMC.chainscat","page":"API","title":"AbstractMCMC.chainscat","text":"chainscat(c::AbstractChains...)\n\nConcatenate multiple chains.\n\nBy default, the chains are concatenated along the third dimension by calling cat(c...; dims=3).\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractMCMC.chainsstack","page":"API","title":"AbstractMCMC.chainsstack","text":"chainsstack(c::AbstractVector)\n\nStack chains in c.\n\nBy default, the vector of chains is returned unmodified. If eltype(c) <: AbstractChains, then reduce(chainscat, c) is called.\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractMCMC.getparams","page":"API","title":"AbstractMCMC.getparams","text":"getparams([model::AbstractModel, ]state)::Vector{<:Real}\n\nRetrieve the values of parameters from the sampler's state as a Vector{<:Real}.\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractMCMC.setparams!!","page":"API","title":"AbstractMCMC.setparams!!","text":"setparams!!([model::AbstractModel, ]state, params)\n\nSet the values of parameters in the sampler's state from a Vector{<:Real}.\n\nThis function should follow the BangBang interface: mutate state in-place if possible and return the mutated state. Otherwise, it should return a new state containing the updated parameters.\n\nAlthough not enforced, it should hold that setparams!!(state, getparams(state)) == state. In other words, the sampler should implement a consistent transformation between its internal representation and the vector representation of the parameter values.\n\nSometimes, to maintain the consistency of the log density and parameter values, a model should be provided. This is useful for samplers that need to evaluate the log density at the new parameter values.\n\n\n\n\n\n","category":"function"},{"location":"design/#Design","page":"Design","title":"Design","text":"This page explains the default implementations and design choices of AbstractMCMC. It is not intended for users but for developers that want to implement the AbstractMCMC interface for Markov chain Monte Carlo sampling. The user-facing API is explained in API.","category":"section"},{"location":"design/#Overview","page":"Design","title":"Overview","text":"AbstractMCMC provides a default implementation of the user-facing interface described in API. You can completely neglect these and define your own implementation of the interface. However, as described below, in most use cases the default implementation allows you to obtain support of parallel sampling, progress logging, callbacks, iterators, and transducers for free by just defining the sampling step of your inference algorithm, drastically reducing the amount of code you have to write. In general, the docstrings of the functions described below might be helpful if you intend to make use of the default implementations.","category":"section"},{"location":"design/#Basic-structure","page":"Design","title":"Basic structure","text":"The simplified structure for regular sampling (the actual implementation contains some additional error checks and support for progress logging and callbacks) is\n\nStatsBase.sample(\n    rng::Random.AbstractRNG,\n    model::AbstractMCMC.AbstractModel,\n    sampler::AbstractMCMC.AbstractSampler,\n    nsamples::Integer;\n    chain_type = ::Type{Any},\n    kwargs...\n)\n    # Obtain the initial sample and state.\n    sample, state = AbstractMCMC.step(rng, model, sampler; kwargs...)\n\n    # Save the sample.\n    samples = AbstractMCMC.samples(sample, model, sampler, N; kwargs...)\n    samples = AbstractMCMC.save!!(samples, sample, 1, model, sampler, N; kwargs...)\n\n    # Step through the sampler.\n    for i in 2:N\n        # Obtain the next sample and state.\n        sample, state = AbstractMCMC.step(rng, model, sampler, state; kwargs...)\n\n        # Save the sample.\n        samples = AbstractMCMC.save!!(samples, sample, i, model, sampler, N; kwargs...)\n    end\n\n    return AbstractMCMC.bundle_samples(samples, model, sampler, state, chain_type; kwargs...)\nend\n\nAll other default implementations make use of the same structure and in particular call the same methods.","category":"section"},{"location":"design/#Sampling-step","page":"Design","title":"Sampling step","text":"The only method for which no default implementation is provided (and hence which downstream packages have to implement) is AbstractMCMC.step. It defines the sampling step of the inference method.\n\nIf one also has some special handling of the warmup-stage of sampling, then this can be specified by overloading\n\nwhich will be used for the first num_warmup iterations, as specified as a keyword argument to AbstractMCMC.sample.  Note that this is optional; by default it simply calls AbstractMCMC.step from above.","category":"section"},{"location":"design/#Collecting-samples","page":"Design","title":"Collecting samples","text":"note: Note\nThis section does not apply to the iterator and transducer interface.\n\nAfter the initial sample is obtained, the default implementations for regular and parallel sampling (not for the iterator and the transducer since it is not needed there) create a container for all samples (the initial one and all subsequent samples) using AbstractMCMC.samples.\n\nIn each step, the sample is saved in the container by AbstractMCMC.save!!. The notation !! follows the convention of the package BangBang.jl which is used in the default implementation of AbstractMCMC.save!!. It indicates that the sample is pushed to the container but a \"widening\" fallback is used if the container type does not allow saving the sample. Therefore AbstractMCMC.save!! always has to return the container.\n\nFor most use cases the default implementation of AbstractMCMC.samples and AbstractMCMC.save!! should work out of the box and hence need not be overloaded in downstream code.","category":"section"},{"location":"design/#Creating-chains","page":"Design","title":"Creating chains","text":"note: Note\nThis section does not apply to the iterator and transducer interface.\n\nAt the end of the sampling procedure for regular and parallel sampling we transform the collection of samples to the desired output type by calling AbstractMCMC.bundle_samples.\n\nThe default implementation should be fine in most use cases, but downstream packages could, e.g., save the final state of the sampler as well if they overload AbstractMCMC.bundle_samples.","category":"section"},{"location":"design/#AbstractMCMC.step","page":"Design","title":"AbstractMCMC.step","text":"step(rng, model, sampler[, state]; kwargs...)\n\nReturn a 2-tuple of the next sample and the next state of the MCMC sampler for model.\n\nSamples describe the results of a single step of the sampler. As an example, a sample might include a vector of parameters sampled from a prior distribution.\n\nWhen sampling using sample, every step call after the first has access to the current state of the sampler.\n\nKeyword arguments\n\nIf the step being taken is going to be discarded (e.g. during burn-in, or if thinning is performed), this method will be called with a discard_sample=true keyword argument. Conversely, if the step being taken is to be retained, this method will be called with discard_sample=false. This allows implementations of step to customize their behavior based on whether or not the sample will be kept.\n\nOther keyword arguments are passed through from the call to sample. Because there is no way of knowing in advance which keyword arguments will be passed, implementations of step should include a kwargs... argument to capture any additional keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"design/#AbstractMCMC.step_warmup","page":"Design","title":"AbstractMCMC.step_warmup","text":"step_warmup(rng, model, sampler[, state]; kwargs...)\n\nReturn a 2-tuple of the next sample and the next state of the MCMC sampler for model.\n\nWhen sampling using sample, this takes the place of AbstractMCMC.step in the first num_warmup number of iterations, as specified by the num_warmup keyword to sample. This is useful if the sampler has an initial \"warmup\"-stage that is different from the standard iteration.\n\nBy default, this defers to AbstractMCMC.step, meaning that if a sampler does not have special warmup behaviour, it only needs to implement step.\n\nKeyword arguments\n\nThe total number of warmup steps requested in sampling will be passed to the step_warmup function as the num_warmup keyword argument. This allows implementations of step_warmup to customise their behavior based on this information.\n\nIf the step being taken is going to be discarded (e.g. during burn-in, or if thinning is performed), this method will be called with a discard_sample=true keyword argument. Conversely, if the step being taken is to be retained, this method will be called with discard_sample=false. This allows implementations of step_warmup to customize their behavior based on whether or not the sample will be kept.\n\nOther keyword arguments are passed through from the call to sample. Because there is no way of knowing in advance which keyword arguments will be passed, implementations of step_warmup should include a kwargs... argument to capture any additional keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"design/#AbstractMCMC.samples","page":"Design","title":"AbstractMCMC.samples","text":"samples(sample, model, sampler[, N; kwargs...])\n\nGenerate a container for the samples of the MCMC sampler for the model, whose first sample is sample.\n\nThe method can be called with and without a predefined number N of samples.\n\n\n\n\n\n","category":"function"},{"location":"design/#AbstractMCMC.save!!","page":"Design","title":"AbstractMCMC.save!!","text":"save!!(samples, sample, iteration, model, sampler[, N; kwargs...])\n\nSave the sample of the MCMC sampler at the current iteration in the container of samples.\n\nThe function can be called with and without a predefined number N of samples. By default, AbstractMCMC uses push!! from the Julia package BangBang to append to the container, and widen its type if needed.\n\n\n\n\n\n","category":"function"},{"location":"design/#AbstractMCMC.bundle_samples","page":"Design","title":"AbstractMCMC.bundle_samples","text":"bundle_samples(samples, model, sampler, state, chain_type[; kwargs...])\n\nBundle all samples that were sampled from the model with the given sampler in a chain.\n\nThe final state of the sampler can be included in the chain. The type of the chain can be specified with the chain_type argument.\n\nBy default, this method returns samples.\n\n\n\n\n\n","category":"function"},{"location":"callbacks/#Callbacks","page":"Callbacks","title":"Callbacks","text":"AbstractMCMC provides a unified callback API for monitoring and logging MCMC sampling.","category":"section"},{"location":"callbacks/#Basic-Usage","page":"Callbacks","title":"Basic Usage","text":"The mcmc_callback function is the main entry point for creating callbacks:\n\nusing AbstractMCMC\n\nstruct MyModel <: AbstractMCMC.AbstractModel end\n\nstruct MySampler <: AbstractMCMC.AbstractSampler end\n\nfunction AbstractMCMC.step(rng, ::MyModel, ::MySampler, state=nothing; kwargs...)\n   # all samples are zero\n   return 0.0, state\nend\n\nmodel, sampler = MyModel(), MySampler()\n\n# Simple callback with a function\ncb = mcmc_callback() do rng, model, sampler, transition, state, iteration\n    println(\"Iteration: $iteration\")\nend\n\nchain = sample(model, sampler, 10; callback=cb)","category":"section"},{"location":"callbacks/#Combining-Multiple-Callbacks","page":"Callbacks","title":"Combining Multiple Callbacks","text":"Pass multiple callbacks to mcmc_callback to combine them:\n\ncb1 = (args...; kwargs...) -> println(\"Callback 1\")\ncb2 = (args...; kwargs...) -> println(\"Callback 2\")\n\ncb = mcmc_callback(cb1, cb2)\n\nYou can also add callbacks dynamically using BangBang.push!!:\n\nusing BangBang\n\ncb = mcmc_callback(cb1)\ncb = push!!(cb, cb2)","category":"section"},{"location":"callbacks/#TensorBoard-Logging","page":"Callbacks","title":"TensorBoard Logging","text":"TensorBoard logging requires TensorBoardLogger. Statistics collection also requires OnlineStats.","category":"section"},{"location":"callbacks/#Basic-Logging-(No-Statistics)","page":"Callbacks","title":"Basic Logging (No Statistics)","text":"using AbstractMCMC\nusing TensorBoardLogger\n\nlogger = TBLogger(\"runs/experiment1\")\ncb = mcmc_callback(logger=logger)\n\nchain = sample(model, sampler, 1000; callback=cb)","category":"section"},{"location":"callbacks/#Logging-with-Statistics","page":"Callbacks","title":"Logging with Statistics","text":"To collect running statistics (mean, variance, histograms), load OnlineStats and use the stats argument:\n\nusing AbstractMCMC\nusing TensorBoardLogger\nusing OnlineStats\n\nlogger = TBLogger(\"runs/experiment1\")\n\n# Use default statistics (Mean, Variance, KHist)\ncb = mcmc_callback(logger=logger, stats=true)\n\n# Or specify custom statistics\ncb = mcmc_callback(\n    logger=logger,\n    stats=(Mean(), Variance(), KHist(50)),\n)\n\nnote: Note\nIf you request statistics without loading OnlineStats, you will get a helpful error: \"Statistics collection requires OnlineStats.jl. Please load OnlineStats before enabling statistics.\"","category":"section"},{"location":"callbacks/#Stats-Processing-Options","page":"Callbacks","title":"Stats Processing Options","text":"Control how samples are processed before computing statistics with stats_options:\n\ncb = mcmc_callback(\n    logger=logger,\n    stats=true,\n    stats_options=(\n        skip=100,    # Skip first 100 samples (burn-in)\n        thin=5,      # Use every 5th sample\n        window=1000, # Rolling window of 1000 samples\n    ),\n)\n\nOptions merge with defaults, so you only need to specify what you want to change:\n\n# Only change thin, skip and window use defaults (0 and typemax(Int))\ncb = mcmc_callback(logger=logger, stats=true, stats_options=(thin=10,))","category":"section"},{"location":"callbacks/#Name-Filtering","page":"Callbacks","title":"Name Filtering","text":"Use name_filter to control which parameters and statistics are logged:\n\ncb = mcmc_callback(\n    logger=logger,\n    name_filter=(\n        include=[\"mu\", \"sigma\"],  # Only log these parameters\n        exclude=[\"_internal\"],     # Exclude matching names\n        stats=false,               # Include step-level statistics\n        extras=true,               # Include extra diagnostics\n    ),\n)","category":"section"},{"location":"callbacks/#Complete-Example","page":"Callbacks","title":"Complete Example","text":"using AbstractMCMC\nusing TensorBoardLogger\nusing OnlineStats\n\nlogger = TBLogger(\"runs/full_example\")\n\ncb = mcmc_callback(\n    logger=logger,\n    stats=true,\n    stats_options=(skip=50, thin=2),\n    name_filter=(\n        exclude=[\"_internal\"],\n        stats=false,\n        extras=true,\n    ),\n)\n\nchain = sample(model, sampler, 10000; callback=cb)\n\nThen view in TensorBoard:\n\ntensorboard --logdir=runs/full_example\n\nNavigate to localhost:6006 in your browser to see the dashboard. You'll see real-time plots of your parameter distributions, histograms, and other statistics as sampling progresses.\n\n(Image: TensorBoard Time Series Tab)\n\nThe Time Series tab provides detailed traces of parameter values throughout the sampling process.\n\n(Image: TensorBoard Scalars Tab)\n\nThe Scalars tab shows time series of parameter values and statistics over the sampling iterations.\n\n(Image: TensorBoard Distributions Tab)\n\nThe Distributions tab displays the marginal distributions of each parameter.\n\n(Image: TensorBoard Histograms Tab)\n\nThe Histograms tab shows the evolution of parameter distributions over time.","category":"section"},{"location":"callbacks/#API-Reference","page":"Callbacks","title":"API Reference","text":"","category":"section"},{"location":"callbacks/#Default-Values","page":"Callbacks","title":"Default Values","text":"","category":"section"},{"location":"callbacks/#stats_options-defaults","page":"Callbacks","title":"stats_options defaults","text":"Option Default Description\nskip 0 Skip first n samples (burn-in)\nthin 0 Use every nth sample (0=all)\nwindow typemax(Int) Window size for rolling stats","category":"section"},{"location":"callbacks/#name_filter-defaults","page":"Callbacks","title":"name_filter defaults","text":"Option Default Description\ninclude String[] Only log these (empty=all)\nexclude String[] Don't log these\nstats false Include step-level statistics\nextras false Include extra diagnostics","category":"section"},{"location":"callbacks/#Implementing-Custom-Callbacks","page":"Callbacks","title":"Implementing Custom Callbacks","text":"Any callable with the following signature can be used as a callback:\n\nfunction my_callback(rng, model, sampler, transition, state, iteration; kwargs...)\n    # Your callback logic here\nend","category":"section"},{"location":"callbacks/#ParamsWithStats","page":"Callbacks","title":"ParamsWithStats","text":"ParamsWithStats is a container for extracting and iterating over MCMC parameters, statistics, and extras.","category":"section"},{"location":"callbacks/#Basic-Usage-2","page":"Callbacks","title":"Basic Usage","text":"# Extract params and stats from state\npws = ParamsWithStats(model, sampler, transition, state; params=true, stats=true)\n\n# Iterate using Base.pairs\nfor (name, value) in Base.pairs(pws)\n    @info name value\nend\n\n# Re-select to get only params\npws_params = ParamsWithStats(pws; params=true, stats=false, extras=false)","category":"section"},{"location":"callbacks/#Overriding-for-Your-Package","page":"Callbacks","title":"Overriding for Your Package","text":"To provide meaningful variable names, override the extraction hooks:\n\n# Option 1: Return Vector{<:Real} - default names (θ[1], θ[2], ...) will be used\nfunction AbstractMCMC.getparams(state::MyState)\n    return [state.mu, state.sigma]\nend\n\n# Option 2: Return named pairs - will be converted to NamedTuple\nfunction AbstractMCMC.getparams(state::MyState)\n    return [\"μ\" => state.mu, \"σ\" => state.sigma]\nend\n\n# Override getstats to return step-level statistics as NamedTuple\nfunction AbstractMCMC.getstats(state::MyState)\n    return (lp=state.logp, acceptance_rate=state.accept_rate)\nend\n\nThe ParamsWithStats constructors normalize all inputs to NamedTuple:\n\nVector{<:Real} gets default θ[i] names\nVector{Pair} is converted to NamedTuple with the provided names\nNamedTuple is used directly\n\nnote: stats vs extras\nUse stats for values that change once per MCMC iteration (e.g., log probability, acceptance rate). Use extras for values that are constant across iterations (e.g., preconditioning matrix, number of particles) or that change multiple times within a single iteration (e.g., leapfrog phase points).","category":"section"},{"location":"callbacks/#Usage-in-TensorBoard-Callback","page":"Callbacks","title":"Usage in TensorBoard Callback","text":"The TensorBoard callback uses ParamsWithStats with Base.pairs:\n\npws = ParamsWithStats(model, sampler, t, state; params=true, stats=true)\nfor (k, val) in Base.pairs(pws)\n    @info \"$k\" val\nend","category":"section"},{"location":"callbacks/#Internals","page":"Callbacks","title":"Internals","text":"note: Note\nThese types and methods are used internally. They are not part of the public API and  may change or break at any time without notice.","category":"section"},{"location":"callbacks/#Types","page":"Callbacks","title":"Types","text":"","category":"section"},{"location":"callbacks/#OnlineStats-Wrappers","page":"Callbacks","title":"OnlineStats Wrappers","text":"When using statistics, AbstractMCMC provides wrappers that modify how samples are processed:\n\nWrapper Description\nSkip(n, stat) Skip first n observations before fitting stat\nThin(n, stat) Only fit every n-th observation to stat\nWindowStat(n, stat) Use a rolling window of n observations\n\nThese are applied automatically via stats_options, but can also be used directly if needed.","category":"section"},{"location":"callbacks/#AbstractMCMC.mcmc_callback","page":"Callbacks","title":"AbstractMCMC.mcmc_callback","text":"mcmc_callback(;\n    logger,\n    stats = nothing,\n    stats_options = nothing,\n    name_filter = nothing,\n)\n\nCreate a TensorBoard logging callback. Requires TensorBoardLogger.jl to be loaded.\n\nArguments\n\nlogger: An AbstractLogger instance (e.g., TBLogger from TensorBoardLogger.jl)\nstats: Statistics to collect. Can be:\nnothing: No statistics (default)\ntrue or :default: Use default statistics (Mean, Variance, KHist) - requires OnlineStats\nAn OnlineStat or tuple of OnlineStats - requires OnlineStats\nstats_options: NamedTuple with thin, skip, window\nname_filter: NamedTuple with include, exclude, stats, extras\n\nExamples\n\nusing TensorBoardLogger\nlg = TBLogger(\"runs/exp\")\ncb = mcmc_callback(logger=lg)\n\n# With default stats (requires OnlineStats)\nusing TensorBoardLogger, OnlineStats\nlg = TBLogger(\"runs/exp\")\ncb = mcmc_callback(logger=lg, stats=true)\n\nnote: Note\nThis method is defined in the TensorBoardLogger extension. You must load TensorBoardLogger before using it: using TensorBoardLogger\n\n\n\n\n\n","category":"function"},{"location":"callbacks/#AbstractMCMC.ParamsWithStats","page":"Callbacks","title":"AbstractMCMC.ParamsWithStats","text":"ParamsWithStats{P,S,E}\n\nA container for MCMC parameters, statistics, and extras.\n\nAll fields are stored as NamedTuples to ensure a tight, well-defined interface. Use Base.pairs(pws) to iterate over (name, value) pairs.\n\nFields\n\nparams::P: Parameter values as a NamedTuple\nstats::S: Statistics as a NamedTuple (e.g., (lp=...,))\nextras::E: Extra diagnostics as a NamedTuple\n\nExample\n\npws = ParamsWithStats(model, sampler, transition, state; params=true, stats=true)\nfor (name, value) in Base.pairs(pws)\n    println(\"$name: $value\")\nend\n\n# Re-select to exclude stats:\npws2 = ParamsWithStats(pws; params=true, stats=false)\n\n\n\n\n\n","category":"type"},{"location":"callbacks/#AbstractMCMC.MultiCallback","page":"Callbacks","title":"AbstractMCMC.MultiCallback","text":"MultiCallback\n\nA callback that combines multiple callbacks into one.\n\nSupports push!! from BangBang.jl to add callbacks, returning a new MultiCallback with the added callback.\n\n\n\n\n\n","category":"type"},{"location":"callbacks/#AbstractMCMC.NameFilter","page":"Callbacks","title":"AbstractMCMC.NameFilter","text":"NameFilter(; include=Set{String}(), exclude=Set{String}())\n\nA filter for variable names.\n\nIf include is non-empty, only names in include will pass the filter.\nNames in exclude will be excluded.\nThrows an error if include and exclude have overlapping elements.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractMCMC.jl","page":"Home","title":"AbstractMCMC.jl","text":"Abstract types and interfaces for Markov chain Monte Carlo methods.\n\nAbstractMCMC defines an interface for sampling and combining Markov chains. It comes with a default sampling algorithm that provides support of progress bars, parallel sampling (multithreaded and multicore), and user-provided callbacks out of the box. Typically developers only have to define the sampling step of their inference method in an iterator-like fashion to make use of this functionality. Additionally, the package defines an iterator and a transducer for sampling Markov chains based on the interface.","category":"section"}]
}
